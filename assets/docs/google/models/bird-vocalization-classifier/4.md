# Module google/bird-vocalization-classifier/4
Google Bird Vocalization Classifier:
A global bird embedding and classification model.

<!-- asset-path: internal -->
<!-- dataset: xeno-canto -->
<!-- task: audio-event-classification -->
<!-- fine-tunable: false -->
<!-- format: saved_model_2 -->
<!-- network-architecture: efficientnet-b1 -->
<!-- license: apache-2.0 -->
<!-- colab: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/hub/tutorials/bird_vocalization_classifier.ipynb -->


## Overview

This model is trained on Xeno-Canto recordings of bird vocalizations.
It provides output logits over more than 10k bird species, and also creates
embedding vectors which can be used for other tasks.

Note that the embedding is the real goal here; the model's logit outputs are
uncalibrated, which might make interpretation difficult in some cases.

### Model Quality

We have evaluated the model on an array of soundscape datasets. For evaluation,
we restrict the logits to a feasible set of classes for the target dataset.
(This prevents misclassification as out-of-domain species.) For each recording,
we use 5s sliding windows with no overlap, and add labels to each audio segment
with any overlap with a GT label. We evaluate classifier performance
on these windowed segments.

Note that the metrics are all uncalibrated, with no dependence on choice of
threshold.

#### Metrics

* ROC-AUC is usually interpreted as the area under the curve of the
  sensitivity/specificity curve. It can also be interpreted as the probability
  that a uniformly-randomly chosen positive example is ranked about a uniformly-
  randomly chosen negative example, and is thus somewhat resistant to both
  positive/negative label imbalance and missing positive labels in the test set.

* cMAP5 computes the mAP of each class independently over the entire dataset,
  and then averages over classes. Classes with fewer than five examples in the
  evaluation set are excluded, to reduce metric noise. Note that cMAP can be
  heavily impacted by missing labels in the test dataset, and the ratio of positive
  and negative labels in each class may also have a subtle effect on cMAP.

* MAP is the mean average precision, computed per-example. It is subject to
  class imbalance.

* Top-1 accuracy computes the accuracy of the highest logit, per example. (Note
  that this provides little insight into examples with multiple vocalizations.)

#### Datasets

* **Caples** is an unreleased dataset collected by the California Academy of
  Science at the Caples Creek area in the central Californian Sierra Nevadas.
  Work is underway to open-source this dataset.

* [**CoffeeFarms**](https://zenodo.org/record/7525349#.ZB8z_-xudhE)
  is a collection of annotated soundscapes from neotropical coffee farms in
  Colombia and Costa Rica.  Part of the data was previously used in the test set
  for the BirdCLEF 2019 competition.

* [**Hawai’i**](https://zenodo.org/record/7078499#.Y7ijPuxudhE)
  (Navine et al., 2022) contains soundscape recordings from Hawai’i,
  USA. Many species, particularly endangered honeycreepers, are endemic to
  Hawai’i and many are under-represented in the Xeno-Canto training set.

* [**High Sierras**](https://zenodo.org/record/7525805#.ZB8zsexudhE) is a
  soundscape dataset of birds from high altitudes in the
  Sierra Nevadas in California. Previously used as part of the test set for the
  Kaggle Cornell Birdcall Identification challenge. Recordings are typically
  sparse, but with very low SNR due to wind noise. Work is underway to
  open-source this dataset.

* [**Peru**](https://zenodo.org/record/7079124#.Y7iis-xudhE) is a dataset of
  recordings from the Amazon rainforest.

* [**Powdermill**](https://zenodo.org/record/4656848#.Y7ijhOxudhE)
  (Chronister et al, 2021) contains high-activity dawn chorus recordings
  captured over four days in Pennsylvania, USA.

* [**Sapsucker Woods**](https://zenodo.org/record/7079380#.Y7ijHOxudhE) (SSW)
  contains soundscape recordings from the Sapsucker Woods bird sanctuary in
  Ithaca, NY, USA.

* [**Sierra Nevada**](https://zenodo.org/record/7050014#.Y7ijWexudhE)
  (Kahl et al., 2022b) contains soundscape recordings from the Sierra Nevadas
  in California, USA.

|              | ROCAUC | cMAP5 | MAP  | Top-1 | #Species | Climate | Hours | XC/Species | Low Data |
|--------------|--------|-------|------|-------|----------|---------|-------|------------|----------|
| CoffeeFarms  | 0.89   | 0.39  | 0.66 | 0.64  |  89      |Tropical |  34   | -          |  ?%      |
| Caples       | 0.86   | 0.26  | 0.59 | 0.57  |  78      |Temperate|   6   | 334.8      | 10%      |
| Hawaii       | 0.78   | 0.30  | 0.52 | 0.51  |  27      |Tropical |  51   | 166.3      | 44%      |
| High Sierras | 0.88   | 0.52  | 0.67 | 0.60  |  34      |Alpine   |  34   | 323.5      |  5%      |
| Peru         | 0.71   | 0.19  | 0.33 | 0.48  | 132      |Tropical |  21   | --         |  ?%      |
| Powdermill   | 0.83   | 0.36  | 0.64 | 0.86  |  48      |Temperate|   6   | 360.0      |  0%      |
| SSW          | 0.89   | 0.27  | 0.58 | 0.57  |  96      |Temperate| 285   | 367.9      |  3%      |
| Sierras      | 0.82   | 0.31  | 0.64 | 0.70  |  56      |Temperate|  33   | 416.0      |  0%      |

### Model Description

The current version of the model uses and EfficientNet-B1 architecture.

The model is trained on Xeno-Canto recordings. We have excluded recordings with
a no-derivative license, and recordings for species on the IUCN Red List.

### Inputs

The input to the model is a batch of 5-second audio segments, sampled at 32kHz.

### Outputs

The model outputs a logits vector and an embedding vector. The eBird species
label associated with each column can be found in the `label.csv` file.

### Usage

While the models may be used as standard TF SavedModels, we suggest using
our [inference wrappers](https://github.com/google-research/chirp/tree/main/chirp/inference).
These will automatically load label class lists, and facilitate restriction to
species subsets.

### Example using TFHub Lib

```python
import tensorflow_hub as hub

# Load the model.
model = hub.load('https://tfhub.dev/google/bird-vocalization-classifier/4')

# Input: 5 seconds of silence as mono 32 kHz waveform samples.
waveform = np.zeros(5 * 32000, dtype=np.float32)

# Run the model, check the output.
logits, embeddings = model.infer_tf(waveform[np.newaxis, :])
```

### Example using Chirp lib

```python
from chirp.inference import models

# Input: 5 seconds of silence as mono 32 kHz waveform samples.
waveform = np.zeros(5 * 32000, dtype=np.float32)

model = models.TaxonomyModelTF(SAVED_MODEL_PATH, 5.0, 5.0)
outputs = model.embed(audio)
# do something with outputs.embeddings and outputs.logits['label']
```

### Changelog

* 1.3 - (version 4 on tfhub) It seems 1.2 didn't include the batched SavedModel.
     So, this new version is exactly the same, but with the batched SavedModel.

* 1.2 - (version 3 on tfhub) Exact same model as 1.1, but with batch processing
    enabled, which we find gives a ~2x speedup per-example on CPU and ~3x
    speedup on a 3090 GPU.
    We have checked that the model outputs match exactly on various examples,
    so there is no need to re-run the model to take advantage of the speedups.

    We also update the reported metrics. We have switched to using sliding-windows
    instead of peak detection for evaluation. Because there are no other model
    changes in this release, one can compare the metrics from the previous release
    to see the impact of this change (mostly the reported scores went down a bit).

    We also now report class-averaged ROC-AUC metrics for each eval dataset,
    after finding that cMAP is subtly sensitive to pos/neg balance within each
    class, and can be very negatively impacted by missing positive ground truth
    labels.

* 1.1 - (version 2 on tfhub) Updated spectrogram ops, reducing kernel size
    dramatically. This yields a ~2x speedup on CPU, with no impact on model
    quality. Replaced some unpublished datasets (Colombia, High Sierras) with
    their newly published versions, and updated the model stats table.
    Note that some stats which seem to decrease since the previous version were affected by a switch to the published versions of datasets. In these cases
    (like SSW), we re-ran evaluation for both the new and old model and
    confirmed there were minimal changes.

* 1.0 - Initial Release

### License

Copyright 2023 Google, LLC

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
