# Module google/vila/image/1

VILA model trained on the AVA dataset. This module includes only the image
branch for predicting the aesthetic quality score.

<!-- asset-path: internal -->
<!-- task: image-aesthetic-quality -->
<!-- fine-tunable: false -->
<!-- format: saved_model_2 -->
<!-- network-architecture: transformer -->
<!-- dataset: ava -->

## Overview
VILA leverages vision-language pretraining on image and user comment pairs for
learning image aesthetics. The exported VILA model predicts image aesthetic
quality score.

This model is exported using the released checkpoints from the CVPR 2023 paper:
[VILA: Learning Image Aesthetics from User Comments with Vision-Language
Pretraining](https://openaccess.thecvf.com/content/CVPR2023/html/Ke_VILA_Learning_Image_Aesthetics_From_User_Comments_With_Vision-Language_Pretraining_CVPR_2023_paper.html)
trained on [AVA dataset](https://ieeexplore.ieee.org/document/6247954)

### Input

A JPEG/PNG image bytes string.

### Output
A dictionary of predictions containing

-   **quality_scores**: (1, 1) quality score range in `[0, 1]`.

### Example use

```python
import tensorflow_hub as hub

image_bytes = load_image('image.png')
model = hub.load('https://tfhub.dev/google/vila/image/1')
predict_fn = model.signatures['serving_default']

predictions = predict_fn(tf.constant(image_bytes))
aesthetic_score = predictions['quality_scores']
```

### Citation

If you find this model useful for your publication, please cite the original
paper:

```
@inproceedings{ke2023vila,
  title = {VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining},
  author={Ke, Junjie and Ye, Keren and Yu, Jiahui and Wu, Yonghui and Milanfar, Peyman and Yang, Feng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10041--10051},
  year={2023}
}
```
